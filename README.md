<center>
  <h2>
    <a href="https://github.com/myshasolin/recommender_systems">
      здесь некоторые упражнения и самостоятельные проекты, выполненные в рамках курса "Фреймворк Apache Spark", от простого (1) к сложному (7): 
    </a>
  </h1>
</center>

<table style="border: 2px double;">
  <tr>
    <th></th>
    <th>тема</th>
    <th>описание задания</th>
    <th>решение</th>
  </tr>
  <tr>
    <td>
      1
    </td>
    <td>
      <a href="https://github.com/myshasolin/Apache_Spark_framework/blob/main/1%20Apache%20Spark.%20%D0%97%D0%BD%D0%B0%D0%BA%D0%BE%D0%BC%D1%81%D1%82%D0%B2%D0%BE/work_1.ipynb">
        Apache Spark. Знакомство
      </a>
    </td>
    <td>
      создать подключение
    </td>
    <td>
      не вопрос
    </td>
  </tr>
  <tr>
    <td>
      2
    </td>
    <td>
      <a href="https://github.com/myshasolin/Apache_Spark_framework/blob/main/2%20%D0%90%D1%80%D1%85%D0%B8%D1%82%D0%B5%D0%BA%D1%82%D1%83%D1%80%D0%B0%20Spark.%20%D0%9F%D1%80%D0%B8%D0%BD%D1%86%D0%B8%D0%BF%D1%8B%20%D0%B8%D1%81%D0%BF%D0%BE%D0%BB%D0%BD%D0%B5%D0%BD%D0%B8%D1%8F%20%D0%B7%D0%B0%D0%BF%D1%80%D0%BE%D1%81%D0%BE%D0%B2.%20%D0%A1%D0%BE%D1%85%D1%80%D0%B0%D0%BD%D0%B5%D0%BD%D0%B8%D0%B5%20%D0%B8%20%D1%87%D1%82%D0%B5%D0%BD%D0%B8%D0%B5%20%D0%B4%D0%B0%D0%BD%D0%BD%D1%8B%D1%85/work_2.ipynb">
        Архитектура Spark. Принципы исполнения запросов. Сохранение и чтение данных
      </a>
    </td>
    <td>
      прочитать файлы в RDD, провести их анализ
    </td>
    <td>
      Файлы train и test качаю с облака, удаляю заголовки, а далее отрисовываю для каждого признака по 2 графика – это отсортированное распределение значений в train и test.<br>Основной код положил в функцию. А там, где для графиков слишком много значений и смотрятся они не очень, применяю «ручную» группировку значений по 100, 100 или 200 шт. в один столбец.<br>В конце файла пара слов – это вывод о том, что было сделано.
    </td>
  </tr>
  <tr>
    <td>
      3
    </td>
    <td>
      <a href="https://github.com/myshasolin/Apache_Spark_framework/tree/main/3%20%D0%9F%D1%80%D0%B8%D0%BD%D1%86%D0%B8%D0%BF%D1%8B%20%D0%B8%D1%81%D0%BF%D0%BE%D0%BB%D0%BD%D0%B5%D0%BD%D0%B8%D1%8F%20%D0%B7%D0%B0%D0%BF%D1%80%D0%BE%D1%81%D0%BE%D0%B2.%20%D0%A1%D0%BE%D1%85%D1%80%D0%B0%D0%BD%D0%B5%D0%BD%D0%B8%D0%B5%20%D0%B8%20%D1%87%D1%82%D0%B5%D0%BD%D0%B8%D0%B5%20%D0%B4%D0%B0%D0%BD%D0%BD%D1%8B%D1%85">
        Принципы исполнения запросов. Сохранение и чтение данных
      </a>
    </td>
    <td>
      1.<br>Требуется выяснить:<br>- Какое соотношение сторон экрана телефона самое популярное<br>- Плотность пикселей у экрана<br>Можно использовать только RDD<br><br>2.<br>Повторить первое задание, но с использованием Spark DataFrame
    </td>
    <td>
      Для DRR:<br>- функция beautiful_print разделяет строку с заголовками по заданному количеству<br>- получение размера RDD оформил в функцию get_size, чтоб код не дублировался<br>- добавление новых столбцов делаю через рукописные функции add_ratio и add_pixel_density<br>- рисую графики соотношения сторон и плотность пикселей<br>- рукописной функцией get_top_ten вывожу на экран ТОП-10 по PPI и соотношению сторон<br><br>Для DataFrame:<br>- в задании показал, как сделать из Spark DataFrame Pandas DataFrame, в Pandas применить pd.concat(), а потом перевести всё обратно из Pandas DataFrame в Spark DataFrame. Получилось тоже самое, что и при spark.unionByName(), только дольше))<br>- добавляю два новых столбца pixel_density и aspect_ratio<br>- вывожу по ним сгруппированные ТОП-10 популярных значений. Результат получился тот же, что и при RDD, только для при PPI= 469 два значения куда-то убежали. Погуглил, в интернетах пишут, что такое возможно из-за разных реализаций RDD и DataFrame
    </td>
  </tr>
  <tr>
    <td>
      4
    </td>
    <td>
      <a href="https://github.com/myshasolin/Apache_Spark_framework/blob/main/4%20%D0%A1%D0%BE%D0%B7%D0%B4%D0%B0%D0%BD%D0%B8%D0%B5%20DataFrame%20%D0%BF%D0%BE%20%D1%81%D1%85%D0%B5%D0%BC%D0%B5%20%D0%B4%D0%B0%D0%BD%D0%BD%D1%8B%D1%85.%20%D0%A2%D1%80%D0%B0%D0%BD%D1%81%D1%84%D0%BE%D1%80%D0%BC%D0%B0%D1%86%D0%B8%D0%B8%20DataFram%D0%BE%D0%B2%20(join%D1%8B%2C%20%D0%B0%D0%B3%D1%80%D0%B5%D0%B3%D0%B0%D1%82%D1%8B%2C%20%D1%8D%D0%BA%D1%81%D0%BF%D0%BE%D1%80%D1%82)/work_4.ipynb">
        Создание DataFrame по схеме данных. Трансформации DataFramов (joinы, агрегаты, экспорт)
      </a>
    </td>
    <td>
      1. Добавьте к таблице следующие поля:<br>- Средняя стомость 10 проданных домов до текущего в том же районе (4digit postcode)<br>- Средняя стомость 10 проданных домов после текущего в том же районе (4digit postcode)<br>- Стоимость последнего проданного дома до текущего<br><br>2. В итоге у вас таблица с колонками: price, среднегодовая цена, средняя стомость 10 проданных домов до текущего в том же районе (4digit postcode), средняя стомость 10 проданных домов после текущего в том же районе (4digit postcode), стоимость последнего проданного дома до текущего и др.<br>- Посчитайте кол-во уникальных значений в каждой строчке (unique(row)) (ипользуйте udf)<br>- Попробуйте сделать то же самое используя pandas udf.<br><br>3. Создайте колонку, в которой в которой будет отображаться "+", "-" или "=", если "Средняя стомость 10 проданных домов до текущего в том же районе" больше, меньше или равно "Средняя стомость 10 проданных домов после текущего в том же районе (4digit postcode)", соотвественно<br>Если одно из полей Null, запишите в эту колонку "Нет данных"
    </td>
    <td>
      в задании много закомментированных строк с пояснением функций – такие вот шпаргалки на бкдкщее для себя<br>для второго задания сделал подсчёт уникальных значений, собирая во множество все значения в строке вообще (дата, цены, индекс, количество спален и пр.)
    </td>
  </tr>
  <tr>
    <td>
      5
    </td>
    <td>
      <a href="https://github.com/myshasolin/Apache_Spark_framework/blob/main/5%20%D0%92%D0%B8%D0%B4%D1%8B%20JOIN%20%D0%B2%20Spark/work_5.ipynb">
        Виды JOIN в Spark
      </a>
    </td>
    <td>
      рассмотреть некоторые виды JOIN-ов, дать определения
    </td>
    <td>
      перед кодом добавил коротенькое описание соответствующих JOIN-ов, а в конце описание всех видов JOIN в Spark вообще
    </td>
  </tr>
  <tr>
    <td>
      7
    </td>
    <td>
      <a href="https://github.com/myshasolin/Apache_Spark_framework/tree/main/7%20%D0%9F%D1%80%D0%BE%D0%B4%D0%BE%D0%BB%D0%B6%D0%B5%D0%BD%D0%B8%D0%B5%20%D0%B8%D0%B7%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D1%8F%20API.%20%D0%9E%D0%BA%D0%BE%D0%BD%D0%BD%D1%8B%D0%B5%20%D1%84%D1%83%D0%BD%D0%BA%D1%86%D0%B8%D0%B8.%20%D0%9F%D0%BE%D0%BB%D1%8C%D0%B7%D0%BE%D0%B2%D0%B0%D1%82%D0%B5%D0%BB%D1%8C%D1%81%D0%BA%D0%B8%D0%B5%20%D1%84%D1%83%D0%BD%D0%BA%D1%86%D0%B8%D0%B8%20UDF">
        Продолжение изучения API. Оконные функции. Пользовательские функции UDF
      </a>
    </td>
    <td>
      1.<br>В Notebook скор считается через pandas и sklearn, сделайте это всё на spark + pipelines<br><br>2.<br>Обучить и оценить модель по данным load_boston с использованием сетки параметров ParamGridBuilder и Pipeline. Оценивается модель при помощи MAE
    </td>
    <td>
      1. файл work_7_LinearRegression – вначале идёт код на Pandas, как он есть по заданию. И только после него я создаю Spark-сессию и импортирую нужные модули. Данные те же и все шаги работы с ними те же. Точно так же добавляю в них столбцы begin_month и target, после оставляю только часть признаков, с помощью randomSplit делю DataFrame на train и test, создаю остальные преобразования и прогоняю данные через Pipeline. Оценку accuracy предсказания получаю через MulticlassClassificationEvaluator (можно было бы использовать BinaryClassificationEvaluator, но у него нет accuracy, а только ROC-площади)<br>2. файл work_7_RandomForestRegressor – это задание по датасету load_boston, которого больше нет в scikit_tearn... Пришлось гуглить и искать его в интернетах. Нашёл на Kaggle. В коде для модели я использую случайный лес, сама модель формируется в пайплайне, а параметры под неё подбираются с помощью сетки ParamGridBuilder, довольно долго, но зато эффективно. Итоговое значение MAE для трейна и теста вывожу в конце. Для теста оно получилось лучше, чем с дефолтными характеристиками леса из примера.
    </td>
  </tr>
</table>
