<center>
  <h2>
    <a href="https://github.com/myshasolin/recommender_systems">
      здесь упражнения и самостоятельные проекты, выполненные в рамках курса "Фреймворк Apache Spark", от простого (1) к сложному (7): 
    </a>
  </h1>
</center>

<table style="border: 2px double;">
  <tr>
    <th></th>
    <th>тема</th>
    <th>описание задания</th>
    <th>решение</th>
  </tr>
  <tr>
    <td>
      1
    </td>
    <td>
      <a href="#">
        Apache Spark. Знакомство
      </a>
    </td>
    <td>
      создать подключение
    </td>
    <td>
      не вопрос
    </td>
  </tr>
  <tr>
    <td>
      2
    </td>
    <td>
      <a href="#">
        Архитектура Spark. Принципы исполнения запросов. Сохранение и чтение данных
      </a>
    </td>
    <td>
      прочитать файлы в RDD, провести их анализ
    </td>
    <td>
      Файлы train и test качаю с облака, удаляю заголовки, а далее отрисовываю для каждого признака по 2 графика – это отсортированное распределение значений в train и test.<br>Основной код положил в функцию. А там, где для графиков слишком много значений и смотрятся они не очень, применяю «ручную» группировку значений по 100, 100 или 200 шт. в один столбец.<br>В конце файла пара слов – это вывод о том, что было сделано.
    </td>
  </tr>
  <tr>
    <td>
      3
    </td>
    <td>
      <a href="#">
        Принципы исполнения запросов. Сохранение и чтение данных
      </a>
    </td>
    <td>
      1.<br>Требуется выяснить:<br>- Какое соотношение сторон экрана телефона самое популярное<br>- Плотность пикселей у экрана<br>Можно использовать только RDD<br><br>2.<br>Повторить первое задание, но с использованием Spark DataFrame
    </td>
    <td>
      Для DRR:<br>- функция beautiful_print разделяет строку с заголовками по заданному количеству<br>- получение размера RDD оформил в функцию get_size, чтоб код не дублировался<br>- добавление новых столбцов делаю через рукописные функции add_ratio и add_pixel_density<br>- рисую графики соотношения сторон и плотность пикселей<br>- рукописной функцией get_top_ten вывожу на экран ТОП-10 по PPI и соотношению сторон<br><br>Для DataFrame:<br>- в задании показал, как сделать из Spark DataFrame Pandas DataFrame, в Pandas применить pd.concat(), а потом перевести всё обратно из Pandas DataFrame в Spark DataFrame. Получилось тоже самое, что и при spark.unionByName(), только дольше))<br>- добавляю два новых столбца pixel_density и aspect_ratio<br>- вывожу по ним сгруппированные ТОП-10 популярных значений. Результат получился тот же, что и при RDD, только для при PPI= 469 два значения куда-то убежали. Погуглил, в интернетах пишут, что такое возможно из-за разных реализаций RDD и DataFrame
    </td>
  </tr>
  <tr>
    <td>
      4
    </td>
    <td>
      <a href="#">
        Создание DataFrame по схеме данных. Трансформации DataFramов (joinы, агрегаты, экспорт)
      </a>
    </td>
    <td>
      1.<br>Добавьте к таблице следующие поля:<br>- Средняя стомость 10 проданных домов до текущего в том же районе (4digit postcode)<br>- Средняя стомость 10 проданных домов после текущего в том же районе (4digit postcode)<br>- Стоимость последнего проданного дома до текущего<br><br>2.<br>В итоге у вас таблица с колонками: price, среднегодовая цена, средняя стомость 10 проданных домов до текущего в том же районе (4digit postcode), средняя стомость 10 проданных домов после текущего в том же районе (4digit postcode), стоимость последнего проданного дома до текущего и др.<br>- Посчитайте кол-во уникальных значений в каждой строчке (unique(row)) (ипользуйте udf)<br>- Попробуйте сделать то же самое используя pandas udf.<br><br>3.<br>Создайте колонку, в которой в которой будет отображаться "+", "-" или "=", если "Средняя стомость 10 проданных домов до текущего в том же районе" больше, меньше или равно "Средняя стомость 10 проданных домов после текущего в том же районе (4digit postcode)", соотвественно<br>Если одно из полей Null, запишите в эту колонку "Нет данных"
    </td>
    <td>
      в задании много закомментированных строк с пояснением функций – такие вот шпаргалки на бкдкщее для себя<br>для второго задания сделал подсчёт уникальных значений, собирая во множество все значения в строке вообще (дата, цены, индекс, количество спален и пр.)
    </td>
  </tr>
  <tr>
    <td>
      5
    </td>
    <td>
      <a href="#">
        Виды JOIN в Spark
      </a>
    </td>
    <td>
      рассмотреть некоторые виды JOIN-ов, дать определения
    </td>
    <td>
      перед кодом добавил коротенькое описание соответствующих JOIN-ов, а в конце описание всех видов JOIN в Spark вообще
    </td>
  </tr>
  <tr>
    <td>
      7
    </td>
    <td>
      <a href="#">
        Продолжение изучения API. Оконные функции. Пользовательские функции UDF
      </a>
    </td>
    <td>
      1.<br>В Notebook скор считается через pandas и sklearn, сделайте это всё на spark + pipelines<br><br>2.<br>Обучить и оценить модель по данным load_boston с использованием сетки параметров ParamGridBuilder и Pipeline. Оценивается модель при помощи MAE
    </td>
    <td>
      1. файл dz_7_LinearRegression – вначале идёт код на Pandas, как он есть по заданию. И только после него я создаю Spark-сессию и импортирую нужные модули. Данные те же и все шаги работы с ними те же. Точно так же добавляю в них столбцы begin_month и target, после оставляю только часть признаков, с помощью randomSplit делю DataFrame на train и test, создаю остальные преобразования и прогоняю данные через Pipeline. Оценку accuracy предсказания получаю через MulticlassClassificationEvaluator (можно было бы использовать BinaryClassificationEvaluator, но у него нет accuracy, а только ROC-площади)<br>2. файл dz_7_RandomForestRegressor – это задание по датасету load_boston, которого больше нет в scikit_tearn... Пришлось гуглить и искать его в интернетах. Нашёл на Kaggle. В коде для модели я использую случайный лес, сама модель формируется в пайплайне, а параметры под неё подбираются с помощью сетки ParamGridBuilder, довольно долго, но зато эффективно. Итоговое значение MAE для трейна и теста вывожу в конце. Для теста оно получилось лучше, чем с дефолтными характеристиками леса из примера.
    </td>
  </tr>
</table>
